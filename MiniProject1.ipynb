{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyPwpNS0Fjh/ZRKAW7SUDh6Z"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["# **Naive Bayes classification :**\n","# **Aim :**\n","\n","The aim of this example is to demonstrate the application of Naive Bayes classification on the MNIST dataset, which consists of handwritten digits, and evaluate the performance of different Naive Bayes variants.\n","\n","# **Title :**\n","\"Naive Bayes Classification on MNIST Handwritten Digits Dataset\"\n","\n","#**Dataset Source :**\n","The MNIST dataset used in this example is a widely used dataset in machine learning and computer vision. It can be obtained from various sources, including the scikit-learn library or online repositories.\n","\n","# **Theory :**\n","Naive Bayes is a probabilistic classification algorithm based on Bayes' theorem with the \"naive\" assumption of feature independence. In the context of this example, we will apply three variants of Naive Bayes classifiers:\n","\n","**Gaussian Naive Bayes (GNB):**\n","\n","Aim: To classify the data assuming features follow a Gaussian distribution.\n","Theory: GNB models feature likelihoods as Gaussian distributions. It is suitable for continuous data, such as the pixel values in the MNIST dataset.\n","Application: We apply GNB to the MNIST dataset to classify handwritten digits.\n","\n","**Multinomial Naive Bayes (MNB):**\n","\n","Aim: To classify the data with non-negative integer or count-like features.\n","Theory: MNB is suitable for discrete data, often used in text classification. It models the probability of a feature given a class as a multinomial distribution.\n","Application: While it's not appropriate for MNIST, we included MNB to highlight the potential error when using it with continuous data.\n","\n","**Bernoulli Naive Bayes (BNB):**\n","\n","Aim: To classify the data with binary features.\n","Theory: BNB is suitable for binary data, where features are binary variables (0s and 1s). It models the probability of features as a set of Bernoulli distributions.\n","Application: BNB is not applied to MNIST in this example because MNIST pixel values are not binary.\n","\n","\n","\n"],"metadata":{"id":"IF4yhsnuXlSK"}},{"cell_type":"code","source":["from sklearn.datasets import fetch_openml\n","from sklearn.model_selection import train_test_split\n","from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score\n","from sklearn.preprocessing import MinMaxScaler"],"metadata":{"id":"U5k2srKoYnHy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the MNIST dataset\n","mnist = fetch_openml(\"mnist_784\")\n","X, y = mnist.data, mnist.target\n","y = y.astype(int)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"-e0l2Z1FYvjN","executionInfo":{"status":"ok","timestamp":1699203002076,"user_tz":-330,"elapsed":53657,"user":{"displayName":"Om Jadhav","userId":"08907511277767535156"}},"outputId":"d67686e5-54fe-4688-a602-bbc4ee128c40"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n","  warn(\n"]}]},{"cell_type":"code","source":["# Apply Min-Max scaling\n","scaler = MinMaxScaler()\n","X = scaler.fit_transform(X)"],"metadata":{"id":"ZyRUFqJOZGjG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"],"metadata":{"id":"qHAOzK7zZMTF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Standardize the features\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)"],"metadata":{"id":"fqyD4zUGZQ2P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Gaussian Naive Bayes\n","gnb = GaussianNB()\n","gnb.fit(X_train, y_train)\n","gnb_predictions = gnb.predict(X_test)\n","gnb_accuracy = accuracy_score(y_test, gnb_predictions)\n","print(\"Gaussian Naive Bayes Accuracy:\", gnb_accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5quszU2jZV6N","executionInfo":{"status":"ok","timestamp":1699203098651,"user_tz":-330,"elapsed":5036,"user":{"displayName":"Om Jadhav","userId":"08907511277767535156"}},"outputId":"8cdac1e0-99bd-4e25-9512-f79939674127"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Gaussian Naive Bayes Accuracy: 0.525\n"]}]},{"cell_type":"code","source":["# Bernoulli Naive Bayes\n","bnb = BernoulliNB()\n","bnb.fit(X_train, y_train)\n","bnb_predictions = bnb.predict(X_test)\n","bnb_accuracy = accuracy_score(y_test, bnb_predictions)\n","print(\"Bernoulli Naive Bayes Accuracy:\", bnb_accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZkIocG2aZhM9","executionInfo":{"status":"ok","timestamp":1699203284198,"user_tz":-330,"elapsed":3021,"user":{"displayName":"Om Jadhav","userId":"08907511277767535156"}},"outputId":"d2e3dc2c-b34e-46d3-c8ed-18845ac2e787"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Bernoulli Naive Bayes Accuracy: 0.834\n"]}]},{"cell_type":"markdown","source":["# **Conclusion:**\n","In this example, we aimed to apply Naive Bayes classification to the MNIST dataset, which contains handwritten digits. We demonstrated the use of the Gaussian Naive Bayes classifier on this continuous data and highlighted the error that can occur when attempting to use Multinomial Naive Bayes on non-negative integer data.\n","\n","The performance of the Gaussian Naive Bayes classifier was measured by accuracy, and it provided a reasonably good performance for classifying the MNIST dataset. The choice of the appropriate Naive Bayes variant depends on the nature of the data, with Gaussian Naive Bayes being a suitable choice for continuous data like MNIST pixel values."],"metadata":{"id":"jVhg1o-6h2El"}},{"cell_type":"markdown","source":["#**Support Vector Machine (SVM) classification**\n","# **Aim :**\n","The aim of this example is to demonstrate the application of Support Vector Machine (SVM) classification on the MNIST dataset, which contains handwritten digits, and to evaluate the performance of different SVM variants, including Linear SVM, Polynomial SVM, and Radial Basis Function (RBF) SVM.\n","\n","# **Title :**\n","\"SVM Classification on MNIST Handwritten Digits Dataset\"\n","\n","# **Dataset Source :**\n","The MNIST dataset used in this example is a widely used dataset in machine learning and computer vision. It can be obtained from various sources, including the scikit-learn library or online repositories.\n","\n","# **Theory :**\n","Support Vector Machine (SVM) is a powerful supervised machine learning algorithm used for classification and regression tasks. In this example, we applied three variants of SVM classifiers:\n","\n","**Linear SVM:**\n","\n","Aim: To classify data using a linear decision boundary.\n","Theory: Linear SVM seeks to find the optimal hyperplane that best separates the data into different classes while maximizing the margin between the classes. It works well when data is linearly separable.\n","Application: Linear SVM is applied to the MNIST dataset to classify handwritten digits using a linear decision boundary.\n","\n","**Polynomial SVM:**\n","\n","Aim: To classify data using a polynomial decision boundary.\n","Theory: Polynomial SVM allows for more complex decision boundaries by applying polynomial kernel functions. It can capture non-linear relationships in the data.\n","Application: Polynomial SVM is applied to the MNIST dataset to classify handwritten digits using polynomial decision boundaries.\n","\n","**Radial Basis Function (RBF) SVM:**\n","\n","Aim: To classify data using an RBF kernel-based decision boundary.\n","Theory: RBF SVM uses a radial basis function kernel to create complex, non-linear decision boundaries. It is highly flexible and can capture intricate patterns in the data.\n","Application: RBF SVM is applied to the MNIST dataset to classify handwritten digits using RBF kernel-based decision boundaries."],"metadata":{"id":"weWrHkBMe7BT"}},{"cell_type":"code","source":["from sklearn.datasets import fetch_openml\n","from sklearn.model_selection import train_test_split\n","from sklearn.svm import SVC\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.metrics import accuracy_score"],"metadata":{"id":"GM9bxeEve70r"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the MNIST dataset\n","mnist = fetch_openml(\"mnist_784\")\n","X, y = mnist.data, mnist.target\n","y = y.astype(int)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tDNNsOHIfOO4","executionInfo":{"status":"ok","timestamp":1699205297840,"user_tz":-330,"elapsed":83882,"user":{"displayName":"Om Jadhav","userId":"08907511277767535156"}},"outputId":"ab9a69c0-e91b-4cfd-ca26-e841917ec373"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n","  warn(\n"]}]},{"cell_type":"code","source":["\n","\n","\n","\n","# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"],"metadata":{"id":"F8VbzLvFfWmQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Standardize the features (optional)\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)"],"metadata":{"id":"UEcwX4rKfb1f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Linear SVM\n","linear_svm = SVC(kernel='linear', C=1.0)\n","linear_svm.fit(X_train[:1000], y_train[:1000])\n","linear_svm_predictions = linear_svm.predict(X_test)\n","linear_svm_accuracy = accuracy_score(y_test, linear_svm_predictions)\n","print(\"Linear SVM Accuracy:\", linear_svm_accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Z-yHDP2Ifg8W","executionInfo":{"status":"ok","timestamp":1699205324657,"user_tz":-330,"elapsed":2921,"user":{"displayName":"Om Jadhav","userId":"08907511277767535156"}},"outputId":"87bf627a-c425-4f7d-c4c7-1f6791c89025"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Linear SVM Accuracy: 0.8907142857142857\n"]}]},{"cell_type":"code","source":["# Polynomial SVM\n","poly_svm = SVC(kernel='poly', degree=3, C=1.0)\n","poly_svm.fit(X_train[:1000], y_train[:1000])\n","poly_svm_predictions = poly_svm.predict(X_test)\n","poly_svm_accuracy = accuracy_score(y_test, poly_svm_predictions)\n","print(\"Polynomial SVM Accuracy:\", poly_svm_accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"l7xMbcu3fkhy","executionInfo":{"status":"ok","timestamp":1699205360404,"user_tz":-330,"elapsed":9818,"user":{"displayName":"Om Jadhav","userId":"08907511277767535156"}},"outputId":"6e535ce2-06cd-4022-d228-9a926b43a7e6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Polynomial SVM Accuracy: 0.479\n"]}]},{"cell_type":"code","source":["# Radial Basis Function (RBF) SVM\n","rbf_svm = SVC(kernel='rbf', C=1.0)\n","rbf_svm.fit(X_train[:1000], y_train[:1000])\n","rbf_svm_predictions = rbf_svm.predict(X_test)\n","rbf_svm_accuracy = accuracy_score(y_test, rbf_svm_predictions)\n","print(\"RBF SVM Accuracy:\", rbf_svm_accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Fc6Av_QefnO_","executionInfo":{"status":"ok","timestamp":1699205376484,"user_tz":-330,"elapsed":9847,"user":{"displayName":"Om Jadhav","userId":"08907511277767535156"}},"outputId":"232108f9-f159-4d19-f1ce-08702b638003"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["RBF SVM Accuracy: 0.8794285714285714\n"]}]},{"cell_type":"markdown","source":["#**Conclusion:**\n","\n","In this example, we aimed to apply Support Vector Machine (SVM) classification to the MNIST dataset, which contains handwritten digits. We demonstrated the use of three different SVM variants (Linear SVM, Polynomial SVM, and RBF SVM) on this dataset.\n","\n","The performance of each SVM variant was measured by accuracy, and we found that the choice of the appropriate SVM variant depends on the complexity of the data and the nature of the decision boundaries. Linear SVM provided a basic classification with a linear decision boundary, while Polynomial SVM and RBF SVM offered more flexibility to capture non-linear patterns in the data.\n","\n","In conclusion, SVMs are versatile classifiers that can be used to classify complex datasets, and the choice of the SVM type depends on the specific characteristics of the data and the problem at hand."],"metadata":{"id":"8AshAxIHhn1m"}},{"cell_type":"markdown","source":["# **Artificial Neural Network (ANN)**\n","# **Aim:**\n","The aim of this example is to demonstrate the application of an Artificial Neural Network (ANN) for classification on the MNIST dataset, which contains handwritten digits, and evaluate its performance.\n","\n","# **Title:**\n","\"Handwritten Digit Classification Using Artificial Neural Networks (ANN) on the MNIST Dataset\"\n","\n","#**Dataset Source:**\n","The MNIST dataset used in this example is a well-known dataset in the field of machine learning and computer vision. It is available from various sources, including the scikit-learn library and online repositories.\n","\n","#**Theory :**\n","Artificial Neural Networks (ANNs) are a class of machine learning models inspired by the human brain. ANNs consist of interconnected layers of artificial neurons that process and transform data. In this example, we applied a feedforward neural network for image classification using the MNIST dataset. Here's an explanation of the key components:\n","\n","**Input Layer:**\n","\n","Aim: To receive input data, which are pixel values of the MNIST images (28x28 pixels).\n","Theory: The input layer has 784 neurons, corresponding to the 28x28 pixel values in each image.\n","\n","**Hidden Layer(s):**\n","\n","Aim: To capture complex patterns and features in the data.\n","Theory: One hidden layer with 128 neurons and another hidden layer with 64 neurons are used. These layers apply activation functions (ReLU) to model non-linear relationships within the data.\n","\n","**Output Layer:**\n","\n","Aim: To produce class predictions for the 10 possible digits (0-9).\n","Theory: The output layer consists of 10 neurons with a softmax activation function, which assigns probabilities to each class, enabling multi-class classification.\n","\n","**Training:**\n","\n","Aim: To adjust the weights of the network to minimize a loss function.\n","Theory: The model is trained using the backpropagation algorithm and the Adam optimizer. It minimizes the sparse categorical cross-entropy loss function.\n","\n","**Evaluation:**\n","\n","Aim: To assess the model's performance.\n","Theory: The model is evaluated on a separate test dataset to calculate accuracy, which measures the percentage of correctly classified digits.\n"],"metadata":{"id":"cODkIJykiOME"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras import Sequential\n","from tensorflow.keras.layers import Dense"],"metadata":{"id":"38DR7xr6iQ0P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the MNIST dataset\n","from sklearn.datasets import fetch_openml\n","from sklearn.model_selection import train_test_split"],"metadata":{"id":"0q5c6N1VjpmO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["mnist = fetch_openml(\"mnist_784\")\n","X, y = mnist.data, mnist.target\n","y = y.astype(int)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lul-OUX1jty4","executionInfo":{"status":"ok","timestamp":1699205947873,"user_tz":-330,"elapsed":66856,"user":{"displayName":"Om Jadhav","userId":"08907511277767535156"}},"outputId":"7e9172f1-89f6-46e4-ec4f-dfc134751ba6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n","  warn(\n"]}]},{"cell_type":"code","source":["# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"],"metadata":{"id":"3Ts36-bFjxv8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Standardize the features (optional)\n","from sklearn.preprocessing import StandardScaler\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)"],"metadata":{"id":"QynOESENj1OV"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Build the ANN model\n","model = Sequential()\n","\n","# Input layer with 784 neurons (MNIST image size)\n","model.add(Dense(128, activation='relu', input_shape=(784,)))\n","\n","# Hidden layer with 64 neurons and ReLU activation\n","model.add(Dense(64, activation='relu'))\n","\n","# Output layer with 10 neurons (for the 10 digits) and softmax activation\n","model.add(Dense(10, activation='softmax'))\n","\n","# Compile the model\n","model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n","\n","# Train the model\n","model.fit(X_train, y_train, epochs=10, batch_size=64, validation_data=(X_test, y_test))\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nCORk6_9j4b4","executionInfo":{"status":"ok","timestamp":1699206042543,"user_tz":-330,"elapsed":84184,"user":{"displayName":"Om Jadhav","userId":"08907511277767535156"}},"outputId":"058a5a3e-771f-4f5e-9a3b-aadd7e70062e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10\n","875/875 [==============================] - 6s 6ms/step - loss: 0.2493 - accuracy: 0.9282 - val_loss: 0.1510 - val_accuracy: 0.9561\n","Epoch 2/10\n","875/875 [==============================] - 5s 5ms/step - loss: 0.0981 - accuracy: 0.9706 - val_loss: 0.1294 - val_accuracy: 0.9636\n","Epoch 3/10\n","875/875 [==============================] - 6s 7ms/step - loss: 0.0659 - accuracy: 0.9793 - val_loss: 0.1216 - val_accuracy: 0.9681\n","Epoch 4/10\n","875/875 [==============================] - 5s 5ms/step - loss: 0.0476 - accuracy: 0.9859 - val_loss: 0.1284 - val_accuracy: 0.9669\n","Epoch 5/10\n","875/875 [==============================] - 4s 5ms/step - loss: 0.0356 - accuracy: 0.9890 - val_loss: 0.1351 - val_accuracy: 0.9695\n","Epoch 6/10\n","875/875 [==============================] - 6s 7ms/step - loss: 0.0331 - accuracy: 0.9897 - val_loss: 0.1494 - val_accuracy: 0.9680\n","Epoch 7/10\n","875/875 [==============================] - 5s 5ms/step - loss: 0.0227 - accuracy: 0.9929 - val_loss: 0.1472 - val_accuracy: 0.9694\n","Epoch 8/10\n","875/875 [==============================] - 6s 7ms/step - loss: 0.0221 - accuracy: 0.9932 - val_loss: 0.1598 - val_accuracy: 0.9706\n","Epoch 9/10\n","875/875 [==============================] - 4s 5ms/step - loss: 0.0207 - accuracy: 0.9929 - val_loss: 0.1738 - val_accuracy: 0.9677\n","Epoch 10/10\n","875/875 [==============================] - 5s 5ms/step - loss: 0.0226 - accuracy: 0.9932 - val_loss: 0.1818 - val_accuracy: 0.9686\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.src.callbacks.History at 0x7f63dc8b7010>"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["\n","\n","\n","# Evaluate the model\n","test_loss, test_accuracy = model.evaluate(X_test, y_test)\n","print(\"Test Accuracy:\", test_accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8sHEea3ij9be","executionInfo":{"status":"ok","timestamp":1699206049932,"user_tz":-330,"elapsed":1706,"user":{"displayName":"Om Jadhav","userId":"08907511277767535156"}},"outputId":"7166af9f-a035-44d5-fe10-0a9eee309ebb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["438/438 [==============================] - 1s 3ms/step - loss: 0.1818 - accuracy: 0.9686\n","Test Accuracy: 0.9685714244842529\n"]}]},{"cell_type":"markdown","source":["#**Conclusion:**\n","In this example, we aimed to apply an Artificial Neural Network (ANN) to the MNIST dataset, a classic handwritten digit classification problem. We used a feedforward neural network architecture with two hidden layers, trained it on the MNIST training data, and evaluated its performance on a test dataset.\n","\n","The ANN model achieved high accuracy in classifying the handwritten digits, demonstrating its ability to learn complex patterns and features from the image data. The choice of network architecture, activation functions, and optimization algorithms can significantly impact the model's performance. ANN models offer flexibility and scalability and can be further fine-tuned to achieve even better results.\n","\n","In conclusion, ANN models are effective for image classification tasks like MNIST, and their performance can be improved with additional architectural modifications and hyperparameter tuning."],"metadata":{"id":"HACpIDkkjYO1"}},{"cell_type":"markdown","source":["#**K-Nearest Neighbor(KNN) Algorithm**\n","# **Theory :**\n","* K-Nearest Neighbour is one of the simplest Machine Learning algorithms based on Supervised Learning technique.\n","* K-NN algorithm assumes the similarity between the new case/data and available cases and put the new case into the category that is most similar to the available categories.\n","* K-NN algorithm stores all the available data and classifies a new data point based on the similarity. This means when new data appears then it can be easily classified into a well suite category by using K- NN algorithm.\n","* K-NN algorithm can be used for Regression as well as for Classification but mostly it is used for the Classification problems.\n","* K-NN is a non-parametric algorithm, which means it does not make any assumption on underlying data.\n","* It is also called a lazy learner algorithm because it does not learn from the training set immediately instead it stores the dataset and at the time of classification, it performs an action on the dataset.\n","* KNN algorithm at the training phase just stores the dataset and when it gets new data, then it classifies that data into a category that is much similar to the new data.\n","* Example: Suppose, we have an image of a creature that looks similar to cat and dog, but we want to know either it is a cat or dog. So for this identification, we can use the KNN algorithm, as it works on a similarity measure. Our KNN model will find the similar features of the new data set to the cats and dogs images and based on the most similar features it will put it in either cat or dog category.\n","\n","#**Decision Tree :**"],"metadata":{"id":"ovECzFw8AIgv"}},{"cell_type":"code","source":["from sklearn.datasets import fetch_openml\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score"],"metadata":{"id":"4HEVm4JvBpXw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the MNIST dataset\n","mnist = fetch_openml(\"mnist_784\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"H3-NovsVB5F5","executionInfo":{"status":"ok","timestamp":1699332444730,"user_tz":-330,"elapsed":61443,"user":{"displayName":"Om Jadhav","userId":"08907511277767535156"}},"outputId":"956f962e-a829-4291-c1fd-362c819af835"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n","  warn(\n"]}]},{"cell_type":"code","source":["X, y = mnist.data, mnist.target\n","y = y.astype(int)"],"metadata":{"id":"mC8RhOnXHEFq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"],"metadata":{"id":"tDS0XLeCCA-a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Standardize the features (optional)\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)"],"metadata":{"id":"FQMVwZ4LCE4v"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Fitting K-NN classifier to the training set\n","from sklearn.neighbors import KNeighborsClassifier\n","knnClassifier= KNeighborsClassifier(n_neighbors=1, metric='minkowski', p=2 )\n","knnClassifier.fit(X_train, y_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":75},"id":"1oM_lP-aCVhU","executionInfo":{"status":"ok","timestamp":1699333546759,"user_tz":-330,"elapsed":1445,"user":{"displayName":"Om Jadhav","userId":"08907511277767535156"}},"outputId":"443c3fa8-4787-489e-c7fd-f04e7470a2cc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["KNeighborsClassifier(n_neighbors=1)"],"text/html":["<style>#sk-container-id-8 {color: black;background-color: white;}#sk-container-id-8 pre{padding: 0;}#sk-container-id-8 div.sk-toggleable {background-color: white;}#sk-container-id-8 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-8 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-8 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-8 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-8 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-8 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-8 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-8 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-8 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-8 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-8 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-8 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-8 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-8 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-8 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-8 div.sk-item {position: relative;z-index: 1;}#sk-container-id-8 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-8 div.sk-item::before, #sk-container-id-8 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-8 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-8 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-8 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-8 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-8 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-8 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-8 div.sk-label-container {text-align: center;}#sk-container-id-8 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-8 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-8\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>KNeighborsClassifier(n_neighbors=1)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" checked><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">KNeighborsClassifier</label><div class=\"sk-toggleable__content\"><pre>KNeighborsClassifier(n_neighbors=1)</pre></div></div></div></div></div>"]},"metadata":{},"execution_count":36}]},{"cell_type":"code","source":["#Predicting the KNN test set result\n","y_predKnn= knnClassifier.predict(X_test)\n","knn_accuracy = accuracy_score(y_test, y_predKnn)\n","print(\"KNeighborsClassifier Accuracy:\", knn_accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RG2xizHYDgUM","executionInfo":{"status":"ok","timestamp":1699333593239,"user_tz":-330,"elapsed":43580,"user":{"displayName":"Om Jadhav","userId":"08907511277767535156"}},"outputId":"9db358d3-c1bb-432f-b367-53de604a28a2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["KNeighborsClassifier Accuracy: 0.9454285714285714\n"]}]},{"cell_type":"code","source":["#Fitting Decision Tree classifier to the training set\n","from sklearn.tree import DecisionTreeClassifier\n","DecClassifier= DecisionTreeClassifier(criterion='entropy', random_state=0)\n","DecClassifier.fit(X_train, y_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":75},"id":"HG3W1AbUIymQ","executionInfo":{"status":"ok","timestamp":1699333614446,"user_tz":-330,"elapsed":17458,"user":{"displayName":"Om Jadhav","userId":"08907511277767535156"}},"outputId":"f4ef147e-f094-478d-cadb-183f275111a0"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["DecisionTreeClassifier(criterion='entropy', random_state=0)"],"text/html":["<style>#sk-container-id-9 {color: black;background-color: white;}#sk-container-id-9 pre{padding: 0;}#sk-container-id-9 div.sk-toggleable {background-color: white;}#sk-container-id-9 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-9 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-9 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-9 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-9 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-9 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-9 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-9 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-9 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-9 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-9 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-9 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-9 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-9 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-9 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-9 div.sk-item {position: relative;z-index: 1;}#sk-container-id-9 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-9 div.sk-item::before, #sk-container-id-9 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-9 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-9 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-9 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-9 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-9 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-9 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-9 div.sk-label-container {text-align: center;}#sk-container-id-9 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-9 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-9\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, random_state=0)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" checked><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">DecisionTreeClassifier</label><div class=\"sk-toggleable__content\"><pre>DecisionTreeClassifier(criterion=&#x27;entropy&#x27;, random_state=0)</pre></div></div></div></div></div>"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["#Predicting the Decision Tree test set result\n","y_predDec= DecClassifier.predict(X_test)\n","dec_accuracy = accuracy_score(y_test, y_predDec)\n","print(\"DecisionTreeClassifier Accuracy:\", dec_accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3PXIXnxWJmEu","executionInfo":{"status":"ok","timestamp":1699333619048,"user_tz":-330,"elapsed":512,"user":{"displayName":"Om Jadhav","userId":"08907511277767535156"}},"outputId":"01b8a841-2e99-45e0-b27f-11cbbaf3f854"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["DecisionTreeClassifier Accuracy: 0.8792142857142857\n"]}]},{"cell_type":"code","source":["#Creating the Confusion matrix KNN\n","from sklearn.metrics import confusion_matrix\n","cmk= confusion_matrix(y_test, y_predKnn)\n","print(cmk)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vgHm9d1nEN4l","executionInfo":{"status":"ok","timestamp":1699333625512,"user_tz":-330,"elapsed":523,"user":{"displayName":"Om Jadhav","userId":"08907511277767535156"}},"outputId":"ad47437e-a3e0-4a52-fe7f-e61fc9c7d7e4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1312    1    4    3    0    6   14    1    2    0]\n"," [   1 1586    6    0    2    0    1    2    1    1]\n"," [  13   13 1291   19    5    4    9   10   11    5]\n"," [   1    2   11 1344    2   25    1   20   16   11]\n"," [   0    6    8    0 1215    3    6   10    1   46]\n"," [   6    2    1   41    5 1173   17    0   20    8]\n"," [  14    4    1    1    4    8 1361    0    2    1]\n"," [   3   14    5    6   16    1    0 1406    1   51]\n"," [   8   10   10   28    3   36    4   10 1229   19]\n"," [   5    3    5    8   28    4    0   44    4 1319]]\n"]}]},{"cell_type":"code","source":["#Creating the Confusion matrix\n","from sklearn.metrics import confusion_matrix\n","cmd= confusion_matrix(y_test, y_predDec)\n","print(cmd)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"emypQaPUKRnW","executionInfo":{"status":"ok","timestamp":1699333629252,"user_tz":-330,"elapsed":2,"user":{"displayName":"Om Jadhav","userId":"08907511277767535156"}},"outputId":"4f93df03-a0f5-468a-8005-87d50f29bf8a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1237    1   21    4    7   11   19    8   18   17]\n"," [   1 1541    8    8    4    8    1   12   13    4]\n"," [  10   17 1201   31   17   14   22   29   29   10]\n"," [   4   10   44 1206    5   57    8   27   44   28]\n"," [   3    2   10    9 1136    8   20   13   26   68]\n"," [  22   15   13   69   12 1042   30    8   39   23]\n"," [  11    7   23    5   21   28 1261    6   27    7]\n"," [   4   10   38   16   15    7    2 1369    9   33]\n"," [   8   18   22   50   35   28   26   12 1117   41]\n"," [   8    7   14   24   81   31    5   26   25 1199]]\n"]}]},{"cell_type":"code","source":["# Random Forest\n","# importing libraries\n","import numpy as nm\n","import matplotlib.pyplot as mtp\n","import pandas as pd\n","from sklearn.datasets import fetch_openml\n","from sklearn.model_selection import train_test_split\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.metrics import accuracy_score\n","from sklearn.preprocessing import StandardScaler"],"metadata":{"id":"A7dmvoHdFhTQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the MNIST dataset\n","mnist = fetch_openml(\"mnist_784\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8sURKNGdFo8g","executionInfo":{"status":"ok","timestamp":1700993230875,"user_tz":-330,"elapsed":66876,"user":{"displayName":"Om Jadhav","userId":"08907511277767535156"}},"outputId":"171e47e0-ad41-46e0-95ac-1552cbebedde"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.10/dist-packages/sklearn/datasets/_openml.py:968: FutureWarning: The default value of `parser` will change from `'liac-arff'` to `'auto'` in 1.4. You can set `parser='auto'` to silence this warning. Therefore, an `ImportError` will be raised from 1.4 if the dataset is dense and pandas is not installed. Note that the pandas parser may return different data types. See the Notes Section in fetch_openml's API doc for details.\n","  warn(\n"]}]},{"cell_type":"code","source":["X, y = mnist.data, mnist.target\n","y = y.astype(int)"],"metadata":{"id":"QbwK8M2-GCyc"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Split the data into training and testing sets\n","X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"],"metadata":{"id":"Q3Iex5gyGDxo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Standardize the features (optional)\n","scaler = StandardScaler()\n","X_train = scaler.fit_transform(X_train)\n","X_test = scaler.transform(X_test)"],"metadata":{"id":"RiuytfA5GI5m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#Fitting Decision Tree classifier to the training set\n","from sklearn.ensemble import RandomForestClassifier\n","classifier= RandomForestClassifier(n_estimators= 10, criterion=\"entropy\")\n","classifier.fit(X_train, y_train)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":74},"id":"Oqnd_VcRGPe_","executionInfo":{"status":"ok","timestamp":1700993389954,"user_tz":-330,"elapsed":6828,"user":{"displayName":"Om Jadhav","userId":"08907511277767535156"}},"outputId":"d4d20a52-0a42-4800-cff1-ff3d20c42d66"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["RandomForestClassifier(criterion='entropy', n_estimators=10)"],"text/html":["<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>RandomForestClassifier(criterion=&#x27;entropy&#x27;, n_estimators=10)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(criterion=&#x27;entropy&#x27;, n_estimators=10)</pre></div></div></div></div></div>"]},"metadata":{},"execution_count":9}]},{"cell_type":"code","source":["#Predicting the test set result\n","y_pred= classifier.predict(X_test)\n","accuracy = accuracy_score(y_test, y_pred)\n","print(\"RandomForestClassifier Accuracy:\",accuracy)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"taDBYZdhG7h1","executionInfo":{"status":"ok","timestamp":1700993560764,"user_tz":-330,"elapsed":9,"user":{"displayName":"Om Jadhav","userId":"08907511277767535156"}},"outputId":"e80232b9-1dd3-47b4-e585-16e7493349f7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["RandomForestClassifier Accuracy: 0.9441428571428572\n"]}]},{"cell_type":"code","source":["#Creating the Confusion matrix\n","from sklearn.metrics import confusion_matrix\n","cm= confusion_matrix(y_test, y_pred)\n","print(cm)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vb5RLW-6HGKq","executionInfo":{"status":"ok","timestamp":1700993568151,"user_tz":-330,"elapsed":784,"user":{"displayName":"Om Jadhav","userId":"08907511277767535156"}},"outputId":"5d1deb9e-e70b-4c6a-dfb1-03da83d6a0b4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[[1315    0    8    0    1    3    6    1    7    2]\n"," [   0 1581    5    4    1    1    0    4    3    1]\n"," [  10    4 1320    6    6    2    9   11    9    3]\n"," [   4    3   38 1324    1   21    4   12   16   10]\n"," [   5    2    6    1 1230    0    4    4    3   40]\n"," [   8    7    6   49    4 1169    9    2   14    5]\n"," [   8    2    6    0   12   12 1352    1    3    0]\n"," [   3    9   21    4   15    1    0 1421    3   26]\n"," [   4   15   39   32    6   24    7    9 1212    9]\n"," [   7    8    3   22   39   15    3   20    9 1294]]\n"]}]},{"cell_type":"markdown","source":["Aim:\n","The aim of this study is to apply various machine learning algorithms, including Decision Trees, K-Nearest Neighbors (KNN), and Neural Networks (ANN), to the MNIST dataset, which contains grayscale images of handwritten digits (0-9). The objective is to perform handwritten digit classification and evaluate the performance of these algorithms.\n","\n","Title:\n","\"Handwritten Digit Classification Using Multiple Machine Learning Algorithms on the MNIST Dataset\"\n","\n","Dataset Source:\n","The MNIST dataset used in this study is sourced from the MNIST database, a widely used dataset in machine learning research. It comprises 28x28 pixel images of handwritten digits, totaling 70,000 examples, where 60,000 are training samples and 10,000 are test samples. The dataset can be accessed from the scikit-learn library or various online repositories.\n","\n","For a detailed explanation of each algorithm, I'll provide concise and separate theoretical insights for Decision Trees, K-Nearest Neighbors (KNN), and Neural Networks (ANN):\n","\n","Decision Tree:\n","Theory (Explanation of Algorithm):\n","Decision Tree is a supervised learning algorithm used for classification and regression tasks. The algorithm works by partitioning the dataset into subsets based on the values of attributes. Here are key components:\n","\n","Tree Structure:\n","\n","Aim: Create a tree-like structure for classification.\n","Theory: Decision Trees recursively partition the feature space based on the most discriminative features. Nodes represent features, branches depict decision rules, and leaf nodes hold class labels.\n","Splitting Criteria:\n","\n","Aim: Determine the best feature and value for splitting.\n","Theory: Decision Trees use impurity measures like Gini Index or Entropy to find the most informative splits that maximize information gain.\n","Training:\n","\n","Aim: Build the tree by recursively partitioning the data.\n","Theory: The model is trained by splitting data at each node based on feature values. The process continues until reaching a stopping criterion.\n","Prediction:\n","\n","Aim: Classify new instances.\n","Theory: During prediction, new instances traverse the tree from the root node to leaf nodes, following decision rules to predict class labels.\n","K-Nearest Neighbors (KNN):\n","Theory (Explanation of Algorithm):\n","K-Nearest Neighbors is a non-parametric and instance-based learning algorithm for classification.\n","\n","Nearest Neighbor Search:\n","\n","Aim: Classify based on majority neighbors.\n","Theory: KNN classifies an instance by finding k-nearest neighbors based on a distance metric (e.g., Euclidean distance).\n","Decision Rule:\n","\n","Aim: Assign class label based on majority voting.\n","Theory: The class label of the majority of the k-nearest neighbors is assigned to the new instance.\n","Hyperparameter k:\n","\n","Aim: Determine the number of neighbors.\n","Theory: The choice of the hyperparameter k significantly affects the model's performance.\n","Prediction:\n","\n","Aim: Classify new instances.\n","Theory: KNN predicts by selecting the class label based on the majority vote among its k-nearest neighbors."],"metadata":{"id":"zfHl62tuqEx0"}}]}